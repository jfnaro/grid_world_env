{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming for Reinforcement Learning with Gridworld Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook was created to practice applying dynamic programming to reinforcement learning. The notes were mainly intended to help me reflect on what I have learned. If you somehow find this, welcome, and please feel free to contact me at joeynaro@rocketmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_world_env_deterministic import grid_world_env_deterministic as deterministic_enviro\n",
    "from grid_renderer import grid_renderer\n",
    "import turtle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True)#precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a few simple gridworld environments. Below I have used one of these environments as the gridworld I am going to solve. The gridworld environments are in my GitHub repo if you are interested in trying them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 5\n",
    "width = 5\n",
    "hole_id = 4\n",
    "goal_id = 3\n",
    "num_of_acts = 4\n",
    "terminal_state = 5\n",
    "test = deterministic_enviro()\n",
    "state, _, _, _ = test.reset()\n",
    "state = state[0:int(len(state)/2)]\n",
    "state = np.reshape(state, (height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the environment looks like. The black represent holes, and the red represents the goal\n",
    "![image of environment](./images/blank_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first attempt, I have greatly penalized falling in the holes and set the goal to always be zero. Let's see how that goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0. -10.   0.]\n",
      " [-10.   0.   0.   0.   0.]\n",
      " [  0. -10.   0.   0. -10.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0. -10.   0.]]\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros((height, width))\n",
    "hole_locations = state == hole_id\n",
    "goal_location = state == goal_id\n",
    "\n",
    "hole_penalty = -10\n",
    "rewards[hole_locations] = hole_penalty\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the state that will follow each state given an action. This environment is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for start_row in range(height):\n",
    "    actions.append([])\n",
    "    for start_col in range(width):\n",
    "        actions[start_row].append([])\n",
    "        for action in range(num_of_acts):\n",
    "            row = start_row\n",
    "            col = start_col\n",
    "            if action == 0 and row > 0:\n",
    "                row -= 1\n",
    "            elif action == 1 and col < width - 1:\n",
    "                col += 1\n",
    "            elif action == 2 and row < height - 1:\n",
    "                row += 1\n",
    "            elif action == 3 and col > 0:\n",
    "                col -= 1\n",
    "            actions[start_row][start_col].append((col, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting policy will find each action equally likely, except in the terminal goal state, which there is no probability of leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]\n",
      "  [0.   0.   0.   0.  ]]]\n"
     ]
    }
   ],
   "source": [
    "policy = np.zeros((height, width, num_of_acts))\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        if not goal_location[row][col]:\n",
    "            for action in range(num_of_acts):\n",
    "                policy[row][col][action] = 1.0 / num_of_acts\n",
    "                \n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters I will use to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "theta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an implementation of synchronous value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "step  1\n",
      "[[ -0.45  -0.45  -0.45 -10.45  -0.45]\n",
      " [-10.45  -0.45  -0.45  -0.45  -0.45]\n",
      " [ -0.45 -10.45  -0.45  -0.45 -10.45]\n",
      " [ -0.45  -0.45  -0.45  -0.45   0.  ]\n",
      " [ -0.45  -0.45  -0.45 -10.     0.  ]]\n",
      "0.45\n",
      "step  2\n",
      "[[ -0.405  -0.405  -0.405 -10.405  -0.405]\n",
      " [-10.405  -0.405  -0.405  -0.405  -0.405]\n",
      " [ -0.405 -10.405  -0.405  -0.405 -10.   ]\n",
      " [ -0.405  -0.405  -0.405   0.      0.   ]\n",
      " [ -0.405  -0.405  -0.405 -10.      0.   ]]\n",
      "0.405\n",
      "step  3\n",
      "[[ -0.3645  -0.3645  -0.3645 -10.3645  -0.3645]\n",
      " [-10.3645  -0.3645  -0.3645  -0.3645  -0.3645]\n",
      " [ -0.3645 -10.3645  -0.3645   0.     -10.    ]\n",
      " [ -0.3645  -0.3645   0.       0.       0.    ]\n",
      " [ -0.3645  -0.3645  -0.3645 -10.       0.    ]]\n",
      "0.36450000000000005\n",
      "step  4\n",
      "[[ -0.32805  -0.32805  -0.32805 -10.32805  -0.32805]\n",
      " [-10.32805  -0.32805  -0.32805   0.       -0.32805]\n",
      " [ -0.32805 -10.32805   0.        0.      -10.     ]\n",
      " [ -0.32805   0.        0.        0.        0.     ]\n",
      " [ -0.32805  -0.32805   0.      -10.        0.     ]]\n",
      "0.32805000000000006\n",
      "step  5\n",
      "[[ -0.295245  -0.295245  -0.295245 -10.        -0.295245]\n",
      " [-10.295245  -0.295245   0.         0.         0.      ]\n",
      " [ -0.295245 -10.         0.         0.       -10.      ]\n",
      " [  0.         0.         0.         0.         0.      ]\n",
      " [ -0.295245   0.         0.       -10.         0.      ]]\n",
      "0.2952450000000001\n",
      "step  6\n",
      "[[ -0.2657205  -0.2657205   0.        -10.          0.       ]\n",
      " [-10.2657205   0.          0.          0.          0.       ]\n",
      " [  0.        -10.          0.          0.        -10.       ]\n",
      " [  0.          0.          0.          0.          0.       ]\n",
      " [  0.          0.          0.        -10.          0.       ]]\n",
      "0.26572050000000047\n",
      "step  7\n",
      "[[ -0.23914845   0.           0.         -10.           0.        ]\n",
      " [-10.           0.           0.           0.           0.        ]\n",
      " [  0.         -10.           0.           0.         -10.        ]\n",
      " [  0.           0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.         -10.           0.        ]]\n",
      "0.23914845000000007\n",
      "step  8\n",
      "[[  0.   0.   0. -10.   0.]\n",
      " [-10.   0.   0.   0.   0.]\n",
      " [  0. -10.   0.   0. -10.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0. -10.   0.]]\n",
      "0.0\n",
      "step  9\n",
      "[[  0.   0.   0. -10.   0.]\n",
      " [-10.   0.   0.   0.   0.]\n",
      " [  0. -10.   0.   0. -10.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0. -10.   0.]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.array(values)\n",
    "\n",
    "synchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if not goal_location[row][col]:\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col] + (gamma * old_values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', synchronous_step)\n",
    "    synchronous_step += 1\n",
    "    \n",
    "    old_values = np.array(values)\n",
    "    values = np.array(rewards)\n",
    "\n",
    "    print(old_values)\n",
    "    \n",
    "values = old_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results, while not great, are certainly interesting. Because of the values I provided, the algorithm has calculated that it is good to reach the goal, but everywhere else is similarly pleasant so long as the agent is not falling into a hole. You can also see where the changes that would happen outside of the bottom right fall below the tolerance, theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can check these numbers with linear algebra, though that would be prohibitively resource expensive on a larger scale. The other problem is that the matrix, 1 - the policy matrix, must be invertible. Let's see if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank:  20\n",
      "Matrix is singular\n"
     ]
    }
   ],
   "source": [
    "lin_al_rewards = np.ndarray.flatten(rewards)\n",
    "lin_al_policy = np.zeros((height * width, height * width))\n",
    "\n",
    "start_index = 0\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        if (not hole_locations[row][col]) and (not goal_location[row][col]):\n",
    "            for next_spot in actions[row][col]:\n",
    "                next_index = (width * next_spot[1]) + next_spot[0]\n",
    "                lin_al_policy[start_index][next_index] += (1.0 / num_of_acts)\n",
    "        start_index += 1\n",
    "\n",
    "print('Rank: ', np.linalg.matrix_rank(1 - lin_al_policy))\n",
    "if (lin_al_policy.shape[0] == lin_al_policy.shape[1]) and (np.linalg.matrix_rank(1 - lin_al_policy) == (height * width)):\n",
    "    lin_al_values = np.matmul(np.linalg.inv(1 - (gamma * lin_al_policy)), lin_al_returns)\n",
    "    print(lin_al_values)\n",
    "else:\n",
    "    print('Matrix is singular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While disappointing, it this not unexpected, because of the environment's states. Let's try something new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that in my first attempt, the agent went to the safety of the goal to avoid being near the holes in most cases. However, the agent calculated that the best thing to do in the lower left was to stay there indefinitely. To penalize this, I want to add a penalty for every step taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.  -1.  -1. -10.  -1.]\n",
      " [-10.  -1.  -1.  -1.  -1.]\n",
      " [ -1. -10.  -1.  -1. -10.]\n",
      " [ -1.  -1.  -1.  -1.  -1.]\n",
      " [ -1.  -1.  -1. -10.   0.]]\n"
     ]
    }
   ],
   "source": [
    "step_penalty = -1\n",
    "\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        if (not hole_locations[row][col]) and (not goal_location[row][col]):\n",
    "            rewards[row][col] = step_penalty\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "step  1\n",
      "[[ -1.45  -1.45  -1.45 -10.45  -1.45]\n",
      " [-10.45  -1.45  -1.45  -1.45  -1.45]\n",
      " [ -1.45 -10.45  -1.45  -1.45 -10.45]\n",
      " [ -1.45  -1.45  -1.45  -1.45  -1.  ]\n",
      " [ -1.45  -1.45  -1.45 -10.     0.  ]]\n",
      "0.8550000000000004\n",
      "step  2\n",
      "[[ -2.305  -2.305  -2.305 -11.305  -2.305]\n",
      " [-11.305  -2.305  -2.305  -2.305  -2.305]\n",
      " [ -2.305 -11.305  -2.305  -2.305 -10.9  ]\n",
      " [ -2.305  -2.305  -2.305  -1.9    -1.   ]\n",
      " [ -2.305  -2.305  -2.305 -10.      0.   ]]\n",
      "0.7695000000000007\n",
      "step  3\n",
      "[[ -3.0745  -3.0745  -3.0745 -12.0745  -3.0745]\n",
      " [-12.0745  -3.0745  -3.0745  -3.0745  -3.0745]\n",
      " [ -3.0745 -12.0745  -3.0745  -2.71   -10.9   ]\n",
      " [ -3.0745  -3.0745  -2.71    -1.9     -1.    ]\n",
      " [ -3.0745  -3.0745  -3.0745 -10.       0.    ]]\n",
      "0.6925500000000007\n",
      "step  4\n",
      "[[ -3.76705  -3.76705  -3.76705 -12.76705  -3.76705]\n",
      " [-12.76705  -3.76705  -3.76705  -3.439    -3.76705]\n",
      " [ -3.76705 -12.76705  -3.439    -2.71    -10.9    ]\n",
      " [ -3.76705  -3.439    -2.71     -1.9      -1.     ]\n",
      " [ -3.76705  -3.76705  -3.439   -10.        0.     ]]\n",
      "0.6232949999999997\n",
      "step  5\n",
      "[[ -4.390345  -4.390345  -4.390345 -13.0951    -4.390345]\n",
      " [-13.390345  -4.390345  -4.0951    -3.439     -4.0951  ]\n",
      " [ -4.390345 -13.0951    -3.439     -2.71     -10.9     ]\n",
      " [ -4.0951    -3.439     -2.71      -1.9       -1.      ]\n",
      " [ -4.390345  -4.0951    -3.439    -10.         0.      ]]\n",
      "0.5609655\n",
      "step  6\n",
      "[[ -4.9513105  -4.9513105  -4.68559   -13.0951     -4.68559  ]\n",
      " [-13.9513105  -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.5048689500000005\n",
      "step  7\n",
      "[[ -5.45617945  -5.217031    -4.68559    -13.0951      -4.68559   ]\n",
      " [-14.217031    -4.68559     -4.0951      -3.439       -4.0951    ]\n",
      " [ -4.68559    -13.0951      -3.439       -2.71       -10.9       ]\n",
      " [ -4.0951      -3.439       -2.71        -1.9         -1.        ]\n",
      " [ -4.68559     -4.0951      -3.439      -10.           0.        ]]\n",
      "0.23914845000000007\n",
      "step  8\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.0\n",
      "step  9\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.array(values)\n",
    "\n",
    "synchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if not goal_location[row][col]:\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col] + (gamma * old_values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', synchronous_step)\n",
    "    synchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "    values = np.copy(rewards)\n",
    "\n",
    "    print(old_values)\n",
    "    \n",
    "values = old_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like success, but let's check the policy. The code below gives a greedy policy correlating to the values calculated above. For this policy, and all of my gridworld environments, 0 is up, 1 is right, 2 is down, and 3 is left. The 5's below represent terminal states. Here is a graphical interpretation of it from my gridworld renderer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 2. 5. 2.]\n",
      " [5. 1. 1. 2. 3.]\n",
      " [2. 5. 1. 2. 5.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: put this in a separate file\n",
    "\n",
    "new_policy = np.zeros((height, width))\n",
    "\n",
    "index = 0\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        if (not hole_locations[row][col]) and (not goal_location[row][col]):\n",
    "            max_next_state_val = -100000 #this should never naturally occur\n",
    "            for action in range(num_of_acts):\n",
    "                if (actions[row][col][action][0] != col or actions[row][col][action][1] != row):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    next_state_val = values[next_spot[1]][next_spot[0]]\n",
    "                    if next_state_val > max_next_state_val:\n",
    "                        max_next_state_val = next_state_val\n",
    "                        new_policy[row][col] = action\n",
    "        else:\n",
    "            new_policy[row][col] = terminal_state\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "print(new_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a resounding success. Here's a visual representation of the policy.\n",
    "#TODO. Add policy visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, I've been using synchronous value iteration. I want to see if asynchronous value iteration can do the job in fewer iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8049999999999997\n",
      "step  1\n",
      "[[ -1.45   -1.45   -1.45  -10.45   -1.45 ]\n",
      " [-10.45   -1.45   -1.45   -1.45   -2.305]\n",
      " [ -1.45  -10.45   -1.45   -1.45  -10.45 ]\n",
      " [ -1.45   -1.45   -1.45   -1.45   -1.   ]\n",
      " [ -1.45   -1.45   -2.305 -10.      0.   ]]\n",
      "1.6245\n",
      "step  2\n",
      "[[ -2.305   -2.305   -2.305  -11.305   -3.0745]\n",
      " [-11.305   -2.305   -2.305   -2.305   -3.0745]\n",
      " [ -2.305  -11.305   -2.305   -2.305  -10.9   ]\n",
      " [ -2.305   -2.305   -2.305   -1.9     -1.    ]\n",
      " [ -2.305   -3.0745  -3.0745 -10.       0.    ]]\n",
      "1.4620500000000005\n",
      "step  3\n",
      "[[ -3.0745   -3.0745   -3.0745  -12.0745   -3.76705]\n",
      " [-12.0745   -3.0745   -3.0745   -3.0745   -3.76705]\n",
      " [ -3.0745  -12.0745   -3.0745   -2.71    -10.9    ]\n",
      " [ -3.0745   -3.0745   -2.71     -1.9      -1.     ]\n",
      " [ -3.76705  -3.76705  -3.439   -10.        0.     ]]\n",
      "0.6925500000000007\n",
      "step  4\n",
      "[[ -3.76705   -3.76705   -3.76705  -12.76705   -4.390345]\n",
      " [-12.76705   -3.76705   -3.76705   -3.439     -4.0951  ]\n",
      " [ -3.76705  -12.76705   -3.439     -2.71     -10.9     ]\n",
      " [ -3.76705   -3.439     -2.71      -1.9       -1.      ]\n",
      " [ -4.390345  -4.0951    -3.439    -10.         0.      ]]\n",
      "0.6232949999999997\n",
      "step  5\n",
      "[[ -4.390345  -4.390345  -4.390345 -13.0951    -4.68559 ]\n",
      " [-13.390345  -4.390345  -4.0951    -3.439     -4.0951  ]\n",
      " [ -4.390345 -13.0951    -3.439     -2.71     -10.9     ]\n",
      " [ -4.0951    -3.439     -2.71      -1.9       -1.      ]\n",
      " [ -4.68559   -4.0951    -3.439    -10.         0.      ]]\n",
      "0.5609655\n",
      "step  6\n",
      "[[ -4.9513105  -4.9513105  -4.68559   -13.0951     -4.68559  ]\n",
      " [-13.9513105  -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.5048689500000005\n",
      "step  7\n",
      "[[ -5.45617945  -5.217031    -4.68559    -13.0951      -4.68559   ]\n",
      " [-14.217031    -4.68559     -4.0951      -3.439       -4.0951    ]\n",
      " [ -4.68559    -13.0951      -3.439       -2.71       -10.9       ]\n",
      " [ -4.0951      -3.439       -2.71        -1.9         -1.        ]\n",
      " [ -4.68559     -4.0951      -3.439      -10.           0.        ]]\n",
      "0.23914845000000007\n",
      "step  8\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.0\n",
      "step  9\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.copy(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if not goal_location[row][col]:\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final values for synchronous and asynchronous have come out the same, and it took the same amount of steps. This took me by surprise for a second until I considered the order values are calculated in, right to left and top to bottom, thus visiting the goal last. I'm going to reverse the order and see if there is a difference, which I do expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8049999999999997\n",
      "step  1\n",
      "[[ -2.305  -1.45   -1.45  -10.45   -2.305]\n",
      " [-10.45   -1.45   -1.45   -1.45   -1.45 ]\n",
      " [ -2.305 -10.45   -1.45   -1.45  -10.45 ]\n",
      " [ -1.45   -1.45   -1.45   -1.45   -1.   ]\n",
      " [ -1.45   -1.45   -1.45  -10.      0.   ]]\n",
      "1.6245000000000012\n",
      "step  2\n",
      "[[ -3.76705  -3.0745   -2.305   -11.305    -3.0745 ]\n",
      " [-12.0745   -2.305    -2.305    -2.305    -2.305  ]\n",
      " [ -3.76705 -11.305    -2.305    -2.305   -10.9    ]\n",
      " [ -3.0745   -2.305    -2.305    -1.9      -1.     ]\n",
      " [ -2.305    -2.305    -2.305   -10.        0.     ]]\n",
      "1.4620500000000005\n",
      "step  3\n",
      "[[ -4.9513105  -4.390345   -3.76705   -12.0745     -3.76705  ]\n",
      " [-13.390345   -3.76705    -3.0745     -3.0745     -3.0745   ]\n",
      " [ -4.68559   -12.0745     -3.0745     -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -3.76705    -3.0745     -3.0745    -10.          0.       ]]\n",
      "1.0206000000000004\n",
      "step  4\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.390345 ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -3.76705  ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.3280500000000002\n",
      "step  5\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.0\n",
      "step  6\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.copy(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height-1, -1, -1):\n",
    "        for col in range(width-1, -1, -1):\n",
    "            if not goal_location[row][col]:\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. Asynchronous value iteration has achieved the same thing as synchronous value iteration in two-thirds the about of steps, 6 compared to 9. However, it required being clever about how to implement the scanning. Out of curiosity, I will now implement asynchronous value iteration with pseudo-random ordering but being sure to visit every space once before moving to the next step. I expect this to be somewhere between the last two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8049999999999997\n",
      "step  1\n",
      "[[ -1.45   -1.45   -2.305 -11.305  -1.45 ]\n",
      " [-11.305  -1.45   -1.45   -1.45   -2.305]\n",
      " [ -1.45  -10.45   -1.45   -1.45  -10.9  ]\n",
      " [ -2.305  -1.45   -2.305  -1.45   -1.   ]\n",
      " [ -1.45   -1.45   -1.45  -10.      0.   ]]\n",
      "1.6245000000000012\n",
      "step  2\n",
      "[[ -3.0745  -2.305   -3.0745 -11.305   -3.0745]\n",
      " [-11.305   -2.305   -2.305   -2.305   -3.0745]\n",
      " [ -3.0745 -12.0745  -2.305   -2.71   -10.9   ]\n",
      " [ -3.0745  -2.305   -2.305   -1.9     -1.    ]\n",
      " [ -2.305   -3.0745  -2.305  -10.       0.    ]]\n",
      "1.7901000000000007\n",
      "step  3\n",
      "[[ -4.390345  -3.76705   -3.76705  -13.0951    -3.76705 ]\n",
      " [-12.76705   -3.0745    -3.0745    -3.439     -3.0745  ]\n",
      " [ -3.76705  -12.0745    -3.0745    -2.71     -10.9     ]\n",
      " [ -3.0745    -3.439     -2.71      -1.9       -1.      ]\n",
      " [ -3.76705   -3.76705   -3.0745   -10.         0.      ]]\n",
      "1.0206000000000004\n",
      "step  4\n",
      "[[ -4.390345  -3.76705   -3.76705  -13.0951    -4.68559 ]\n",
      " [-12.76705   -3.76705   -4.0951    -3.439     -4.0951  ]\n",
      " [ -3.76705  -12.76705   -3.439     -2.71     -10.9     ]\n",
      " [ -4.0951    -3.439     -2.71      -1.9       -1.      ]\n",
      " [ -3.76705   -4.0951    -3.439    -10.         0.      ]]\n",
      "0.9185400000000001\n",
      "step  5\n",
      "[[ -4.9513105  -4.390345   -4.68559   -13.0951     -4.68559  ]\n",
      " [-13.390345   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.8266860000000005\n",
      "step  6\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n",
      "0.0\n",
      "step  7\n",
      "[[ -5.6953279  -5.217031   -4.68559   -13.0951     -4.68559  ]\n",
      " [-14.217031   -4.68559    -4.0951     -3.439      -4.0951   ]\n",
      " [ -4.68559   -13.0951     -3.439      -2.71      -10.9      ]\n",
      " [ -4.0951     -3.439      -2.71       -1.9        -1.       ]\n",
      " [ -4.68559    -4.0951     -3.439     -10.          0.       ]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.copy(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "\n",
    "states = [(x,y) for x in range(width) for y in range(height) if not goal_location[y][x]]\n",
    "random.shuffle(states)\n",
    "\n",
    "while delta > theta:\n",
    "    for state in states:\n",
    "        col, row = state\n",
    "        if not goal_location[row][col]:\n",
    "            next_state_vals = []\n",
    "            for action in range(num_of_acts):\n",
    "                next_spot = actions[row][col][action]\n",
    "                if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                    next_val = rewards[row][col] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "                    next_state_vals.append(next_val)\n",
    "            if len(next_state_vals) != 0:\n",
    "                values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "    \n",
    "    random.shuffle(states)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was correct, but I'm still surprised that it only took 7 steps. It makes sense that the previous two examples serve as upper and power bounds for efficiency. As such, one would expect randomness to fall in the middle. This might be worth keeping in mind for less intuitive optimal policies in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that I've done enough with value iteration for now. I'm going to move on to policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-bd2cf6d74763>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-bd2cf6d74763>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    while delta > theta:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values += -0.5\n",
    "values[hole_locations] = hole_penalty\n",
    "values[goal_location] = 0\n",
    "old_values = np.copy(values)\n",
    "\n",
    "synchronous_step = 1\n",
    "\n",
    "old_policy = np.zeros((height, width))\n",
    "old_policy += terminal_state\n",
    "\n",
    "total_policy_evaluations = 0\n",
    "total_policy iterations = 0\n",
    "\n",
    "while not np.array_equal(policy, old_policy):\n",
    "\n",
    "    #policy evaluation\n",
    "    while delta > theta:\n",
    "        for row_2 in range(height):\n",
    "            for col_2 in range(width):\n",
    "                if not goal_location[row_2][col_2]:\n",
    "                    values[row_2][col_2] = 0\n",
    "                    for action in range(num_of_acts):\n",
    "                        next_spot = actions[row_2][col_2][action]\n",
    "                        values[row_2][col_2] += rewards[row_2][col_2] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "\n",
    "        delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "        print(delta)\n",
    "        print('step ', asynchronous_step)\n",
    "        asynchronous_step += 1\n",
    "\n",
    "        old_values = np.copy(values)\n",
    "\n",
    "        print(values)\n",
    "        \n",
    "    #policy improvement\n",
    "    for row_1 in range(height):\n",
    "        for col_1 in range(width):\n",
    "            if (not hole_locations[row_1][col_1]) and (not goal_location[row_1][col_1]):\n",
    "                max_next_state_val = -100000 #this should never naturally occur\n",
    "                for action in range(num_of_acts):\n",
    "                    if (actions[row_1][col_1][action][0] != col or actions[row_1][col_1][action][1] != row):\n",
    "                        next_spot = actions[row_1][col_1][action]\n",
    "                        next_state_val = values[next_spot[1]][next_spot[0]]\n",
    "                        if next_state_val > max_next_state_val:\n",
    "                            max_next_state_val = next_state_val\n",
    "                            policy[row_1][col_1] = action\n",
    "            else:\n",
    "                policy[row][col] = terminal_state\n",
    "\n",
    "                \n",
    "    total_policy_improvements += 1\n",
    "\n",
    "    print('total policy improvements: ', total_policy_improvements)\n",
    "\n",
    "    print(policy)\n",
    "    \n",
    "    synchronous_step += 1\n",
    "    print('total cycles: ', synchronous_step)\n",
    "    \n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
