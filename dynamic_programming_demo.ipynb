{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming for Reinforcement Learning with Gridworld Examples\n",
    "\n",
    "\n",
    "### Topics\n",
    "\n",
    "* Policy Iteration\n",
    "* Value Iteration\n",
    "* Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are my own\n",
    "from grid_world_env_deterministic import grid_world_env_deterministic as deterministic_enviro\n",
    "from grid_renderer import grid_renderer\n",
    "\n",
    "import turtle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True, threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 4. 0.]\n",
      " [4. 0. 0. 0. 0.]\n",
      " [0. 4. 0. 0. 4.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 3.]]\n"
     ]
    }
   ],
   "source": [
    "height = 5\n",
    "width = 5\n",
    "hole_id = 4\n",
    "goal_id = 3\n",
    "blank_id = 0\n",
    "agent_id = 1\n",
    "num_of_acts = 4\n",
    "terminal_state = 5\n",
    "evn_state = 0\n",
    "agent_state = 1\n",
    "hole_penalty = -10\n",
    "step_penalty = -1\n",
    "\n",
    "env = deterministic_enviro()\n",
    "state, _, _, _ = env.reset()\n",
    "state = state[0:int(len(state)/2)]\n",
    "state = np.reshape(state, (height, width))\n",
    "goal_locations = state == goal_id\n",
    "hole_locations = state == hole_id\n",
    "print(state)\n",
    "renderer = grid_renderer(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the environment looks like. The black represent holes, and the red represents the goal\n",
    "![image of environment](./images/blank_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can probe the environment to obtain the following matrix, which gives the rewards for choosing an action from each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -1.  -1. -10.  -1.]\n",
      "  [ -1.  -1.  -1.  -1.]\n",
      "  [ -1. -10.  -1.  -1.]\n",
      "  [-10.  -1.  -1.  -1.]\n",
      "  [ -1.  -1.  -1. -10.]]\n",
      "\n",
      " [[ -1.  -1.  -1. -10.]\n",
      "  [ -1.  -1. -10. -10.]\n",
      "  [ -1.  -1.  -1.  -1.]\n",
      "  [-10.  -1.  -1.  -1.]\n",
      "  [ -1.  -1. -10.  -1.]]\n",
      "\n",
      " [[-10. -10.  -1.  -1.]\n",
      "  [ -1.  -1.  -1.  -1.]\n",
      "  [ -1.  -1.  -1. -10.]\n",
      "  [ -1. -10.  -1.  -1.]\n",
      "  [ -1. -10.  -1.  -1.]]\n",
      "\n",
      " [[ -1.  -1.  -1.  -1.]\n",
      "  [-10.  -1.  -1.  -1.]\n",
      "  [ -1.  -1.  -1.  -1.]\n",
      "  [ -1.  -1. -10.  -1.]\n",
      "  [-10.  -1.   0.  -1.]]\n",
      "\n",
      " [[ -1.  -1.  -1.  -1.]\n",
      "  [ -1.  -1.  -1.  -1.]\n",
      "  [ -1. -10.  -1.  -1.]\n",
      "  [ -1.   0. -10.  -1.]\n",
      "  [ -1.   0.   0. -10.]]]\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros((height, width, num_of_acts))\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        for action in range(num_of_acts):\n",
    "            _, rewards[y][x][action], _, _ = env.hypothetical((x,y), action)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the state that will follow each state given an action. This environment is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for start_row in range(height):\n",
    "    actions.append([])\n",
    "    for start_col in range(width):\n",
    "        actions[start_row].append([])\n",
    "        for action in range(num_of_acts):\n",
    "            row = start_row\n",
    "            col = start_col\n",
    "            if action == 0 and row > 0:\n",
    "                row -= 1\n",
    "            elif action == 1 and col < width - 1:\n",
    "                col += 1\n",
    "            elif action == 2 and row < height - 1:\n",
    "                row += 1\n",
    "            elif action == 3 and col > 0:\n",
    "                col -= 1\n",
    "            actions[start_row][start_col].append((row, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters I will use to start with. Gamma is the discount factor, and theta is how close to no change in values the algorithm will go before it considers policy evaluation complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "theta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy iteration algorithm](./images/policy_iteration_formula_page_80.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy evaluation\n",
    "def policy_evaluation(pol, vals, evaluations):\n",
    "    \n",
    "    delta = 1.0\n",
    "    old_values = np.array(vals)\n",
    "\n",
    "    while delta > theta:\n",
    "        for row_2 in range(height):\n",
    "            for col_2 in range(width):\n",
    "                if not (goal_locations[row_2][col_2] or hole_locations[row_2][col_2]):\n",
    "                    action = pol[row_2][col_2]\n",
    "                    next_spot = actions[row_2][col_2][int(action)]\n",
    "                    #no state transition probability is shown, because the environment is deterministic\n",
    "                    vals[row_2][col_2] = rewards[row_2][col_2] + (gamma * old_values[next_spot[1]][next_spot[0]])\n",
    "\n",
    "        delta = np.amax(abs(np.subtract(old_values, vals)))\n",
    "        evaluations += 1\n",
    "        print('policy evaluation: ', evaluations)\n",
    "\n",
    "        old_values = np.array(vals)\n",
    "\n",
    "        print(vals)\n",
    "        \n",
    "    return vals, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy improvement\n",
    "def policy_improvement(values, improvements):\n",
    "    \n",
    "    pol = np.zeros((height, width))\n",
    "    \n",
    "    for row_1 in range(height):\n",
    "        for col_1 in range(width):\n",
    "            if not goal_locations[row_1][col_1]:\n",
    "                max_next_state_val = -100000 #this should never naturally occur\n",
    "                for action in range(num_of_acts):\n",
    "                    if (actions[row_1][col_1][int(action)][0] != col_1 or actions[row_1][col_1][int(action)][1] != row_1):\n",
    "                        next_spot = actions[row_1][col_1][action]\n",
    "                        next_state_val = values[next_spot[1]][next_spot[0]]\n",
    "                        if next_state_val > max_next_state_val:\n",
    "                            max_next_state_val = next_state_val\n",
    "                            pol[row_1][col_1] = action\n",
    "            else:\n",
    "                pol[row_1][col_1] = terminal_state\n",
    "\n",
    "                \n",
    "    improvements += 1\n",
    "\n",
    "    print('policy improvement: ', improvements)\n",
    "\n",
    "    print(pol)\n",
    "    \n",
    "    return pol, improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    #Values are intialized arbitrarily \n",
    "    values = np.zeros((height, width))\n",
    "    values += step_penalty\n",
    "    values[hole_locations] = hole_penalty\n",
    "    values[goal_locations] = 0\n",
    "\n",
    "    policy = np.zeros((height, width))\n",
    "    old_policy = np.zeros((height, width))\n",
    "    old_policy += terminal_state\n",
    "\n",
    "    total_policy_evaluations = 0\n",
    "    total_policy_improvements = 0\n",
    "\n",
    "    while not np.array_equal(policy, old_policy):\n",
    "\n",
    "        old_policy = np.array(policy)\n",
    "\n",
    "        values, total_policy_evaluations = policy_evaluation(old_policy, values, total_policy_evaluations)\n",
    "\n",
    "        policy, total_policy_improvements = policy_improvement(values, total_policy_improvements)\n",
    "\n",
    "        print(policy)\n",
    "        print(old_policy)\n",
    "\n",
    "    print('total evaluations and improvements: ', total_policy_evaluations + total_policy_improvements)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-addc92ec16a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#uncomment to run turtle visualizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#renderer.grid_policy(policy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7733e1e24651>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mold_policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_policy_evaluations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_policy_evaluations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_policy_improvements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_policy_improvements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-525b78dbb8f4>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[1;34m(pol, vals, evaluations)\u001b[0m\n\u001b[0;32m     12\u001b[0m                     \u001b[0mnext_spot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[1;31m#no state transition probability is shown, because the environment is deterministic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                     \u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mold_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_spot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_spot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "policy = policy_iteration()\n",
    "#uncomment to run turtle visualizer\n",
    "#renderer.grid_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visual representation of the optimal policy determined through policy iteration\n",
    "![optimal policy visualized](./images/optimal_policy_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_height = 3\n",
    "# gamma_width = 3\n",
    "# gamma_env = deterministic_enviro(height=gamma_height, width=gamma_width, hole_quantity=0)\n",
    "# gamma_state, _, _, _ = env.reset()\n",
    "# gamma_state = gamma_state[0:int(len(gamma_state)/2)]\n",
    "# gamma_state = np.reshape(gamma_state, (gamma_height, gamma_width))\n",
    "# goal_locations = gamma_state == goal_id\n",
    "# hole_locations = gamma_state == hole_id\n",
    "# print(gamma_state)\n",
    "# renderer = grid_renderer(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluates the policy 66 times. Let's try something more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual representation of policy iteration](./images/policy_iteration_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual representation of value iteration](./images/value_iteration_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value iteration algorithm](./images/value_iteration_formula_page_83.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an implementation of synchronous value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values[hole_locations] = hole_penalty\n",
    "old_values = np.array(values)\n",
    "\n",
    "synchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if not (goal_locations[row][col] or hole_locations[row][col]):\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col][action] + (gamma * old_values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "                    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', synchronous_step)\n",
    "    synchronous_step += 1\n",
    "    \n",
    "    old_values = np.array(values)\n",
    "    values = np.zeros((height, width))\n",
    "    values[hole_locations] = hole_penalty\n",
    "\n",
    "    print(old_values)\n",
    "    \n",
    "values = old_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like success, but let's check the policy. The code below gives a greedy policy correlating to the values calculated above. For this policy, and all of my gridworld environments, 0 is up, 1 is right, 2 is down, and 3 is left. The 5's below represent terminal states. Here is a graphical interpretation of it from my gridworld renderer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: put this in a separate file\n",
    "\n",
    "new_policy = np.zeros((height, width))\n",
    "\n",
    "index = 0\n",
    "for row in range(height):\n",
    "    for col in range(width):\n",
    "        if not goal_locations[row][col]:\n",
    "            max_next_state_val = -100000 #this should never naturally occur\n",
    "            for action in range(num_of_acts):\n",
    "                if (actions[row][col][action][0] != col or actions[row][col][action][1] != row):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    next_state_val = values[next_spot[1]][next_spot[0]]\n",
    "                    if next_state_val > max_next_state_val:\n",
    "                        max_next_state_val = next_state_val\n",
    "                        new_policy[row][col] = action\n",
    "        else:\n",
    "            new_policy[row][col] = terminal_state\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "print(new_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the optimal policy previously obtained from policy iteration. Policy iteration and value iteration both went through 8 policy improvements. However, under value iteration, there were only 8 policy evaluations under value iteration compared with 66 policy evaluations under policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asychronous Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, I've been using synchronous value iteration. I want to see if asynchronous value iteration can do the job in fewer iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values[hole_locations] = hole_penalty\n",
    "old_values = np.array(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if not (goal_locations[row][col] or hole_locations[row][col]):\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col][action] + (gamma * old_values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final values for synchronous and asynchronous have come out the same, and it took the same amount of steps. This took me by surprise for a second until I considered the order values are calculated in, left to right and top to bottom, thus visiting the goal last. I'm going to reverse the order and see if there is a difference, which I do expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values[hole_locations] = hole_penalty\n",
    "old_values = np.array(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height-1, -1, -1):\n",
    "        for col in range(width-1, -1, -1):\n",
    "            if not (goal_locations[row][col] or hole_locations[row][col]):\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_spot = actions[row][col][action]\n",
    "                    if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                        next_val = rewards[row][col][action] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. Asynchronous value iteration has achieved the same thing as synchronous value iteration in almost half the amount of steps, 5 compared to 8. However, it required being clever about how to implement the scanning. Out of curiosity, I will now implement asynchronous value iteration with pseudo-random ordering but being sure to visit every space once before moving to the next step. I expect this to be somewhere between the last two exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.zeros((height, width))\n",
    "values[hole_locations] = hole_penalty\n",
    "old_values = np.copy(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "\n",
    "states = [(x,y) for x in range(width) for y in range(height) if not goal_locations[y][x]]\n",
    "random.shuffle(states)\n",
    "\n",
    "while delta > theta:\n",
    "    for state in states:\n",
    "        col, row = state\n",
    "        if not (goal_locations[row][col] or hole_locations[row][col]):\n",
    "            next_state_vals = []\n",
    "            for action in range(num_of_acts):\n",
    "                next_spot = actions[row][col][action]\n",
    "                if not (next_spot[0] == col and next_spot[1] == row):\n",
    "                    next_val = rewards[row][col][action] + (gamma * values[next_spot[1]][next_spot[0]])\n",
    "                    next_state_vals.append(next_val)\n",
    "            if len(next_state_vals) != 0:\n",
    "                values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "    \n",
    "    random.shuffle(states)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above block several times, I can confirm that random state selection takes between 6 and 8 steps. It makes sense that the previous two examples serve as upper and power bounds for efficiency. As such, one would expect randomness to fall in the middle. This might be worth keeping in mind for less intuitive optimal policies in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochasctic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 0. 5. 4. 5.]\n",
      " [4. 5. 0. 5. 5.]\n",
      " [5. 4. 5. 5. 4.]\n",
      " [0. 5. 0. 5. 5.]\n",
      " [0. 0. 5. 4. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from grid_world_env_stochastic import grid_world_env_stochastic as stochastic_enviro\n",
    "env = stochastic_enviro()\n",
    "state, _, _, _ = env.reset()\n",
    "state = state[0:int(len(state)/2)]\n",
    "state = np.reshape(state, (height, width))\n",
    "goal_locations = state == goal_id\n",
    "hole_locations = state == hole_id\n",
    "print(state)\n",
    "renderer = grid_renderer(state)\n",
    "#renderer.colored_spots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.zeros((len(state) * len(state[0]), len(state) * len(state[0])))\n",
    "i = 0\n",
    "for row in range(len(state)):\n",
    "    for col in range(len(state[0])):\n",
    "        if state[row][col] in (0, 3, 4):\n",
    "            index = row * len(state[0]) + col \n",
    "            transition_matrix[i][index] = 1.\n",
    "        elif state[row][col] == 5:\n",
    "            remainder = 1.\n",
    "            for act in range(num_of_acts):\n",
    "                n_row, n_col = actions[row][col][act]\n",
    "                if state[n_row][n_col] == 4:\n",
    "                    index = n_row * len(state[0]) + n_col \n",
    "                    transition_matrix[i][index] += 0.25\n",
    "                    remainder -= 0.25\n",
    "            index = row * len(state[0]) + col\n",
    "            transition_matrix[i][index] = remainder\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.175\n",
      "step  1\n",
      "[[ -2.71   -1.9    -1.9   -10.     -6.175]\n",
      " [-10.     -1.9    -6.175  -1.9    -6.175]\n",
      " [ -2.71  -10.     -1.9    -6.175 -10.   ]\n",
      " [ -1.9    -1.9    -6.175  -1.9     0.   ]\n",
      " [ -1.9    -1.9    -1.9   -10.      0.   ]]\n",
      "4.657500000000001\n",
      "step  2\n",
      "[[ -3.439     -2.71      -2.71     -10.        -9.668125]\n",
      " [-10.        -2.71      -6.7825    -6.5575    -6.7825  ]\n",
      " [ -3.439    -10.        -6.5575    -6.7825   -10.      ]\n",
      " [ -2.71      -2.71      -6.7825    -5.5        0.      ]\n",
      " [ -2.71      -2.71      -2.71     -10.         0.      ]]\n",
      "3.1438125\n",
      "step  3\n",
      "[[ -4.0951      -3.439       -3.439      -10.         -12.02598438]\n",
      " [-10.          -3.439       -7.32925     -7.10425     -9.9263125 ]\n",
      " [ -4.0951     -10.          -7.10425     -9.2125     -10.        ]\n",
      " [ -3.439       -3.439       -7.32925     -5.5          0.        ]\n",
      " [ -3.439       -3.439       -3.439      -10.           0.        ]]\n",
      "0.6561000000000003\n",
      "step  4\n",
      "[[ -4.68559     -4.0951      -4.0951     -10.         -12.44937391]\n",
      " [-10.          -4.0951      -7.821325    -7.596325   -10.29536875]\n",
      " [ -4.68559    -10.          -7.596325    -9.2125     -10.        ]\n",
      " [ -4.0951      -4.0951      -7.821325    -5.5          0.        ]\n",
      " [ -4.0951      -4.0951      -4.0951     -10.           0.        ]]\n",
      "0.59049\n",
      "step  5\n",
      "[[ -5.217031    -4.68559     -4.68559    -10.         -12.67357558]\n",
      " [-10.          -4.68559     -8.2641925   -8.0391925  -10.62751938]\n",
      " [ -5.217031   -10.          -8.0391925   -9.2125     -10.        ]\n",
      " [ -4.68559     -4.68559     -8.2641925   -5.5          0.        ]\n",
      " [ -4.68559     -4.68559     -4.68559    -10.           0.        ]]\n",
      "0.531441\n",
      "step  6\n",
      "[[ -5.6953279   -5.217031    -5.217031   -10.         -12.87535708]\n",
      " [-10.          -5.217031    -8.66277325  -8.43777325 -10.92645494]\n",
      " [ -5.6953279  -10.          -8.43777325  -9.2125     -10.        ]\n",
      " [ -5.217031    -5.217031    -8.66277325  -5.5          0.        ]\n",
      " [ -5.217031    -5.217031    -5.217031   -10.           0.        ]]\n",
      "0.47829690000000014\n",
      "step  7\n",
      "[[ -6.12579511  -5.6953279   -5.6953279  -10.         -13.05696044]\n",
      " [-10.          -5.6953279   -9.02149592  -8.79649593 -11.19549694]\n",
      " [ -6.12579511 -10.          -8.79649593  -9.2125     -10.        ]\n",
      " [ -5.6953279   -5.6953279   -9.02149592  -5.5          0.        ]\n",
      " [ -5.6953279   -5.6953279   -5.6953279  -10.           0.        ]]\n",
      "0.43046720999999977\n",
      "step  8\n",
      "[[ -6.5132156   -6.12579511  -6.12579511 -10.         -13.22040346]\n",
      " [-10.          -6.12579511  -9.34434633  -9.11934633 -11.43763475]\n",
      " [ -6.5132156  -10.          -9.11934633  -9.2125     -10.        ]\n",
      " [ -6.12579511  -6.12579511  -9.2125      -5.5          0.        ]\n",
      " [ -6.12579511  -6.12579511  -6.12579511 -10.           0.        ]]\n",
      "0.38742048900000015\n",
      "step  9\n",
      "[[ -6.86189404  -6.5132156   -6.5132156  -10.         -13.36750217]\n",
      " [-10.          -6.5132156   -9.6349117   -9.4099117  -11.65555877]\n",
      " [ -6.86189404 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -6.5132156   -6.5132156   -9.2125      -5.5          0.        ]\n",
      " [ -6.5132156   -6.5132156   -6.5132156  -10.           0.        ]]\n",
      "0.3486784401000005\n",
      "step  10\n",
      "[[ -7.17570464  -6.86189404  -6.86189404 -10.         -13.49989102]\n",
      " [-10.          -6.86189404  -9.89642053  -9.67142053 -11.8516904 ]\n",
      " [ -7.17570464 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -6.86189404  -6.86189404  -9.2125      -5.5          0.        ]\n",
      " [ -6.86189404  -6.86189404  -6.86189404 -10.           0.        ]]\n",
      "0.3138105960899997\n",
      "step  11\n",
      "[[ -7.45813417  -7.17570464  -7.17570464 -10.         -13.61904098]\n",
      " [-10.          -7.17570464 -10.13177848  -9.90677848 -12.02820886]\n",
      " [ -7.45813417 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -7.17570464  -7.17570464  -9.2125      -5.5          0.        ]\n",
      " [ -7.17570464  -7.17570464  -7.17570464 -10.           0.        ]]\n",
      "0.28242953648099967\n",
      "step  12\n",
      "[[ -7.71232075  -7.45813417  -7.45813417 -10.         -13.72627594]\n",
      " [-10.          -7.45813417 -10.34360063 -10.11860063 -12.18707547]\n",
      " [ -7.71232075 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -7.45813417  -7.45813417  -9.2125      -5.5          0.        ]\n",
      " [ -7.45813417  -7.45813417  -7.45813417 -10.           0.        ]]\n",
      "0.25418658283289997\n",
      "step  13\n",
      "[[ -7.94108868  -7.71232075  -7.71232075 -10.         -13.82278741]\n",
      " [-10.          -7.71232075 -10.53424057 -10.30924057 -12.33005542]\n",
      " [ -7.94108868 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -7.71232075  -7.71232075  -9.2125      -5.5          0.        ]\n",
      " [ -7.71232075  -7.71232075  -7.71232075 -10.           0.        ]]\n",
      "0.2287679245496097\n",
      "step  14\n",
      "[[ -8.14697981  -7.94108868  -7.94108868 -10.         -13.90964773]\n",
      " [-10.          -7.94108868 -10.70581651 -10.48081651 -12.45873738]\n",
      " [ -8.14697981 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -7.94108868  -7.94108868  -9.2125      -5.5          0.        ]\n",
      " [ -7.94108868  -7.94108868  -7.94108868 -10.           0.        ]]\n",
      "0.20589113209465015\n",
      "step  15\n",
      "[[ -8.33228183  -8.14697981  -8.14697981 -10.         -13.98782202]\n",
      " [-10.          -8.14697981 -10.86023486 -10.63523486 -12.57455114]\n",
      " [ -8.33228183 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.14697981  -8.14697981  -9.2125      -5.5          0.        ]\n",
      " [ -8.14697981  -8.14697981  -8.14697981 -10.           0.        ]]\n",
      "0.18530201888518505\n",
      "step  16\n",
      "[[ -8.49905365  -8.33228183  -8.33228183 -10.         -14.05817888]\n",
      " [-10.          -8.33228183 -10.99921137 -10.77421137 -12.67878353]\n",
      " [ -8.49905365 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.33228183  -8.33228183  -9.2125      -5.5          0.        ]\n",
      " [ -8.33228183  -8.33228183  -8.33228183 -10.           0.        ]]\n",
      "0.16677181699666477\n",
      "step  17\n",
      "[[ -8.64914828  -8.49905365  -8.49905365 -10.         -14.12150006]\n",
      " [-10.          -8.49905365 -11.12429024 -10.89929024 -12.77259268]\n",
      " [ -8.64914828 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.49905365  -8.49905365  -9.2125      -5.5          0.        ]\n",
      " [ -8.49905365  -8.49905365  -8.49905365 -10.           0.        ]]\n",
      "0.1500946352969983\n",
      "step  18\n",
      "[[ -8.78423345  -8.64914828  -8.64914828 -10.         -14.17848911]\n",
      " [-10.          -8.64914828 -11.23686121 -11.01186121 -12.85702091]\n",
      " [ -8.78423345 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.64914828  -8.64914828  -9.2125      -5.5          0.        ]\n",
      " [ -8.64914828  -8.64914828  -8.64914828 -10.           0.        ]]\n",
      "0.13508517176729917\n",
      "step  19\n",
      "[[ -8.90581011  -8.78423345  -8.78423345 -10.         -14.22977926]\n",
      " [-10.          -8.78423345 -11.33817509 -11.11317509 -12.93300632]\n",
      " [ -8.90581011 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.78423345  -8.78423345  -9.2125      -5.5          0.        ]\n",
      " [ -8.78423345  -8.78423345  -8.78423345 -10.           0.        ]]\n",
      "0.12157665459056943\n",
      "step  20\n",
      "[[ -9.0152291   -8.90581011  -8.90581011 -10.         -14.2759404 ]\n",
      " [-10.          -8.90581011 -11.42935758 -11.20435758 -13.00139319]\n",
      " [ -9.0152291  -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -8.90581011  -8.90581011  -9.2125      -5.5          0.        ]\n",
      " [ -8.90581011  -8.90581011  -8.90581011 -10.           0.        ]]\n",
      "0.10941898913151249\n",
      "step  21\n",
      "[[ -9.11370619  -9.0152291   -9.0152291  -10.         -14.31748542]\n",
      " [-10.          -9.0152291  -11.51142182 -11.28642182 -13.06294137]\n",
      " [ -9.11370619 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -9.0152291   -9.0152291   -9.2125      -5.5          0.        ]\n",
      " [ -9.0152291   -9.0152291   -9.0152291  -10.           0.        ]]\n",
      "0.0984770902183616\n",
      "step  22\n",
      "[[ -9.20233557  -9.11370619  -9.11370619 -10.         -14.35487594]\n",
      " [-10.          -9.11370619 -11.58527964 -11.36027964 -13.11833473]\n",
      " [ -9.20233557 -10.          -9.29125     -9.2125     -10.        ]\n",
      " [ -9.11370619  -9.11370619  -9.2125      -5.5          0.        ]\n",
      " [ -9.11370619  -9.11370619  -9.11370619 -10.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "delta = 1.0\n",
    "gamma = 0.9\n",
    "\n",
    "rewards = np.zeros((height, width))\n",
    "rewards -= 1.\n",
    "rewards[hole_locations] = -10.\n",
    "rewards[goal_locations] = 0.\n",
    "\n",
    "\n",
    "#Values are intialized arbitrarily \n",
    "values = np.copy(rewards)\n",
    "old_values = np.array(values)\n",
    "\n",
    "asynchronous_step = 1\n",
    "while delta > theta:\n",
    "    for row in range(height-1, -1, -1):\n",
    "        for col in range(width-1, -1, -1):\n",
    "            if not (goal_locations[row][col] or hole_locations[row][col]):\n",
    "                next_state_vals = []\n",
    "                for action in range(num_of_acts):\n",
    "                    next_row, next_col = actions[row][col][action]\n",
    "                    if state[next_row][next_col] in (0, 3, 4):\n",
    "                        next_val = rewards[next_row][next_col] + (gamma * values[next_row][next_col])\n",
    "                        next_state_vals.append(next_val)\n",
    "                    else:\n",
    "                        index = (next_row * len(state[0])) + next_col\n",
    "                        next_val = 0\n",
    "                        for s in range(len(state) * len(state[0])):\n",
    "                            if transition_matrix[index][s] > 0:\n",
    "                                r = s // len(state[0])\n",
    "                                c = s % len(state[0])\n",
    "                                test1 = transition_matrix[index][s]\n",
    "                                test2 = rewards[r][c]\n",
    "                                test3 = values[r][c]\n",
    "                                next_val += transition_matrix[index][s] * (rewards[r][c] + (gamma * values[r][c]))\n",
    "                        next_state_vals.append(next_val)\n",
    "                if len(next_state_vals) != 0:\n",
    "                    values[row][col] = max(next_state_vals)\n",
    "    \n",
    "    delta = np.amax(abs(np.subtract(old_values, values)))\n",
    "    print(delta)\n",
    "    print('step ', asynchronous_step)\n",
    "    asynchronous_step += 1\n",
    "    \n",
    "    old_values = np.copy(values)\n",
    "\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
